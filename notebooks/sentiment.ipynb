{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "data_path = r'C:\\Users\\anura\\Desktop\\Project 4- Climate Change Modeling\\data\\raw\\climate_nasa.csv'\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract text data for sentiment analysis\n",
    "data = data.dropna(subset=['text', 'commentsCount'])\n",
    "\n",
    "# Create multiclass labels based on commentsCount\n",
    "def generate_sentiment_label(count):\n",
    "    if count < 0:\n",
    "        return 0  # Negative sentiment\n",
    "    elif count == 0:\n",
    "        return 1  # Neutral sentiment\n",
    "    else:\n",
    "        return 2  # Positive sentiment\n",
    "\n",
    "data['sentiment'] = data['commentsCount'].apply(generate_sentiment_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Preprocess the text\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)  \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split data into training and testing sets\n",
    "X = data['cleaned_text']\n",
    "y = data['sentiment']  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Tokenize text for Word2Vec\n",
    "X_train_tokenized = [word_tokenize(text) for text in X_train]\n",
    "X_test_tokenized = [word_tokenize(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model trained.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X_train_tokenized, vector_size=300, window=5, min_count=1, workers=4)\n",
    "print(\"Word2Vec model trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create sentence embeddings\n",
    "def generate_word2vec_embedding(tokens, word2vec_model):\n",
    "    embeddings = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size) \n",
    "\n",
    "X_train_embeddings = np.array([generate_word2vec_embedding(tokens, word2vec_model) for tokens in X_train_tokenized])\n",
    "X_test_embeddings = np.array([generate_word2vec_embedding(tokens, word2vec_model) for tokens in X_test_tokenized])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Define the RNN Model\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  \n",
    "        _, (hidden, _) = self.rnn(x)\n",
    "        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1) \n",
    "        output = self.fc(hidden_cat)\n",
    "        return self.softmax(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Prepare Data for RNN\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = SentimentDataset(X_train_embeddings, y_train)\n",
    "test_dataset = SentimentDataset(X_test_embeddings, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 7.6590\n",
      "Epoch [2/50], Loss: 7.4934\n",
      "Epoch [3/50], Loss: 7.3010\n",
      "Epoch [4/50], Loss: 7.0644\n",
      "Epoch [5/50], Loss: 6.7676\n",
      "Epoch [6/50], Loss: 6.4020\n",
      "Epoch [7/50], Loss: 5.9783\n",
      "Epoch [8/50], Loss: 5.5206\n",
      "Epoch [9/50], Loss: 5.0835\n",
      "Epoch [10/50], Loss: 4.7159\n",
      "Epoch [11/50], Loss: 4.4428\n",
      "Epoch [12/50], Loss: 4.2583\n",
      "Epoch [13/50], Loss: 4.1390\n",
      "Epoch [14/50], Loss: 4.0630\n",
      "Epoch [15/50], Loss: 4.0137\n",
      "Epoch [16/50], Loss: 3.9818\n",
      "Epoch [17/50], Loss: 3.9592\n",
      "Epoch [18/50], Loss: 3.9425\n",
      "Epoch [19/50], Loss: 3.9306\n",
      "Epoch [20/50], Loss: 3.9208\n",
      "Epoch [21/50], Loss: 3.9137\n",
      "Epoch [22/50], Loss: 3.9067\n",
      "Epoch [23/50], Loss: 3.9024\n",
      "Epoch [24/50], Loss: 3.8981\n",
      "Epoch [25/50], Loss: 3.8947\n",
      "Epoch [26/50], Loss: 3.8916\n",
      "Epoch [27/50], Loss: 3.8891\n",
      "Epoch [28/50], Loss: 3.8867\n",
      "Epoch [29/50], Loss: 3.8847\n",
      "Epoch [30/50], Loss: 3.8828\n",
      "Epoch [31/50], Loss: 3.8813\n",
      "Epoch [32/50], Loss: 3.8798\n",
      "Epoch [33/50], Loss: 3.8785\n",
      "Epoch [34/50], Loss: 3.8774\n",
      "Epoch [35/50], Loss: 3.8763\n",
      "Epoch [36/50], Loss: 3.8753\n",
      "Epoch [37/50], Loss: 3.8744\n",
      "Epoch [38/50], Loss: 3.8736\n",
      "Epoch [39/50], Loss: 3.8729\n",
      "Epoch [40/50], Loss: 3.8723\n",
      "Epoch [41/50], Loss: 3.8716\n",
      "Epoch [42/50], Loss: 3.8710\n",
      "Epoch [43/50], Loss: 3.8704\n",
      "Epoch [44/50], Loss: 3.8700\n",
      "Epoch [45/50], Loss: 3.8695\n",
      "Epoch [46/50], Loss: 3.8690\n",
      "Epoch [47/50], Loss: 3.8687\n",
      "Epoch [48/50], Loss: 3.8683\n",
      "Epoch [49/50], Loss: 3.8679\n",
      "Epoch [50/50], Loss: 3.8676\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Train the RNN\n",
    "input_dim = word2vec_model.vector_size\n",
    "hidden_dim = 128\n",
    "output_dim = 3  \n",
    "\n",
    "model = RNNClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for embeddings, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        49\n",
      "\n",
      "    accuracy                           1.00        49\n",
      "   macro avg       1.00      1.00      1.00        49\n",
      "weighted avg       1.00      1.00      1.00        49\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49]]\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Evaluate the RNN\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in test_loader:\n",
    "        outputs = model(embeddings)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Multiclass Model saved at C:\\Users\\anura\\Desktop\\Project 4- Climate Change Modeling\\models\\rnn_multiclass_sentiment_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Save the trained model\n",
    "model_dir = r'C:\\Users\\anura\\Desktop\\Project 4- Climate Change Modeling\\models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_save_path = os.path.join(model_dir, 'rnn_multiclass_sentiment_model.pth')\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"RNN Multiclass Model saved at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
